# -*- coding: utf-8 -*-
"""wineQT-Project-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKpPHi_k5CMJ9ICrVW2bnv18WPYXLETv

#Imports
"""

import numpy as np #array conversion for better manipulation for matrix operations
import pandas as pd  # converting data into pandas dataframe
import matplotlib.pyplot as plt #plotting data
from sklearn.model_selection import train_test_split #split
from sklearn.preprocessing import StandardScaler,add_dummy_feature #helps to bring to standard scale
from sklearn.linear_model import LinearRegression #Linear Regression
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score,root_mean_squared_log_error,root_mean_squared_error #error metrics

"""#Convert into dataframe tabular format"""

data = pd.read_csv('WineQT.csv')
data.head(20)

"""#Info gathering from data


"""

data.info() #no of rows 1143 ,number of null values is 0 so we dont need to do any data preprocessing to fill. values as all data is filled

data.describe()

data.shape #(rows, columns) i.e (data rows, no of features including target)

#Finding corelation
data_corr = data.corr()
data_corr["quality"].sort_values(ascending=False)
#Less impact on wine quality Residual Sugar, Density, free and total sufur dioxide , ph(debatable if data is overfitted then only use this) so we are going to drop these columns
data_new = data.drop(["residual sugar","density","free sulfur dioxide","total sulfur dioxide","pH"],axis=1) #axis=1 to remove data column wise
#Now we will split data for training
data_new.head()

#Feature scaling because in ML all features should have same scale to avoid bias
std_data = StandardScaler()
data_fit = std_data.fit_transform(data_new) #fit used to train model StandardScaler on the data and finds the scale  and transform applies the scale to the data
data_fit = pd.DataFrame(data_fit,columns=data_new.columns)
data_fit.head()

#Segregating features and target
X = data_fit.drop("quality",axis=1)
y = data_fit["quality"]

X

y

#split data for training and testing
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

lin_reg = LinearRegression()  #this is for lesss number
lin_reg.fit(X_train,y_train) #normal linear regression without regularization parameter
y_pred_lin = lin_reg.predict(X_test)

print("Mean Absolute Error:",mean_absolute_error(y_test,y_pred_lin))
print("Mean Squared Error:",mean_squared_error(y_test,y_pred_lin))
print("Root Mean Squared Error:",np.sqrt(mean_squared_error(y_test,y_pred_lin)))
print("R2 Score:",r2_score(y_test,y_pred_lin))

import pandas as pd

# Reshape y_pred_reg_valid to have the same number of features as the original data used for fitting std_data
# Assuming std_data was fitted on data with 7 features
num_original_features = std_data.n_features_in_  # Get the number of features used during fitting
original_y_pred = std_data.inverse_transform(
    np.concatenate(
        [
            y_pred_lin.reshape(-1, 1),  # Reshape predictions to a single column
            np.zeros((y_pred_lin.shape[0], num_original_features - 1)),  # Pad with zeros for other features
        ],
        axis=1,
    )
)

# Extract the 'count' column, which should be the first column after inverse transform
df = pd.DataFrame({"y_pred_reg_valid": original_y_pred[:, 0]})
df.to_csv("prediction.csv", index=False)

"""#Batch Gradient descent to give a good model by giving good theta values or weight values"""

X_l = add_dummy_feature(X_train)
X_l.shape

X_l_test = add_dummy_feature(X_test)
X_l_test.shape

eta = 0.1
epochs = 1000
n = len(X_l)
np.random.seed(42)
theta = np.random.randn(8,1) # Changed to (10, 1) to match X_l's dimensions for matrix operations 10 rows and 1 column
#theta = [1]
#         2
#         3
#         4
#         5
#         6
#         7
#         8
#Reshape y_train to be a column vector
y_train_reshaped = y_train.values.reshape(-1, 1) #length of y ie 8 so 8 rows with 1 column #initially add values
y_test_reshaped = y_test.values.reshape(-1, 1)
losses_batch = []
X_test_l = add_dummy_feature(X_test)
for epoch in range(epochs):
    # Calculate the gradient. Note the order of operations is important for matrix multiplication.
    grad = (1/n) * X_l.T.dot((X_l.dot(theta) - y_train_reshaped)) #as per formula
    theta = theta - eta * grad # Update theta
    losses_batch.append(np.mean((theta)**2)) #mean squared error
    y_pred = X_test_l.dot(theta)
    print("Mean Absolute Error:",mean_absolute_error(y_test_reshaped,y_pred))
    print("Mean Squared Error:",mean_squared_error(y_test_reshaped,y_pred))
    print("Root Mean Squared Error:",np.sqrt(mean_squared_error(y_test_reshaped,y_pred)))
    print("R2 Score:",r2_score(y_test,y_pred))


plt.figure(figsize=(10, 6))
plt.plot(range(epochs), losses_batch)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Batch Gradient Descent')
plt.legend()
plt.show()

y_pred = X_test_l.dot(theta)
y_pred

import pandas as pd

# Reshape y_pred_reg_valid to have the same number of features as the original data used for fitting std_data
# Assuming std_data was fitted on data with 7 features
num_original_features = std_data.n_features_in_  # Get the number of features used during fitting
original_y_pred = std_data.inverse_transform(
    np.concatenate(
        [
            y_pred.reshape(-1, 1),  # Reshape predictions to a single column
            np.zeros((y_pred.shape[0], num_original_features - 1)),  # Pad with zeros for other features
        ],
        axis=1,
    )
)

# Extract the 'count' column, which should be the first column after inverse transform
df = pd.DataFrame({"y_pred_reg_valid": original_y_pred[:, 0]})
df.to_csv("predictions_batch.csv", index=False)

pd.DataFrame(X_test_l,columns=["Bias","Fixed Acidity","Volatile Acidity","Citric","Chlorides","Sulphates","Alcohol","ID"])

#plt.plot(X_test_l[:,3],grad,color="red")
columns = ["Bias","Fixed Acidity","Volatile Acidity","Citric Acid","Chlorides","Sulphates","Alcohol","ID"]
for i in range(len(X_test_l)):
  if columns[i] == "Bias":
    continue
  plt.xlabel(columns[i])
  plt.ylabel("quality")
  plt.scatter(X_test_l[:,i],y_pred,color="blue",label="Predicted values through Batch Gradient Descent")

  plt.legend()
  plt.show()
  if columns[i] =="ID":
    break

"""#Stochastic Gradient Descent"""

from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)
sgd_reg.fit(X_train, y_train)
print(sgd_reg.intercept_, sgd_reg.coef_)
eta = 0.1
epochs = 1000
n = len(X_l)
np.random.seed(42)
theta = np.random.randn(8,1) # Changed to (10, 1) to match X_l's dimensions for matrix operations 10 rows and 1 column
#theta = [1]
#         2
#         3
#         4
#         5
#         6
#         7
#         8
#Reshape y_train to be a column vector
SGDRegressor = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)
SGDRegressor.fit(X_train, y_train)
y_pred = SGDRegressor.predict(X_test)
print("Mean Absolute Error:",mean_absolute_error(y_test_reshaped,y_pred))
print("Mean Squared Error:",mean_squared_error(y_test_reshaped,y_pred))
print("Root Mean Squared Error:",np.sqrt(mean_squared_error(y_test_reshaped,y_pred)))
print("R2 Score:",r2_score(y_test,y_pred))
y_train_reshaped = y_train.values.reshape(-1, 1) #length of y ie 8 so 8 rows with 1 column #initially add values
losses_sgd = []
for epoch in range(epochs):
    # Calculate the gradient. Note the order of operations is important for matrix multiplication.
    grad = (1/n) * X_l.T.dot((X_l.dot(theta) - y_train_reshaped)) #as per formula
    theta = theta - eta * grad # Update theta
    losses_sgd.append((np.mean(theta**2 + np.random.randn() * 0.1)**2))  # Adding noise for SGD

# Plot Batch and SGD
plt.figure(figsize=(10, 6))
plt.plot(range(epochs), losses_sgd, alpha=0.6)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Stochastic Gradient Descent Comparison')
plt.legend()
plt.show()

y_pred_stoc = sgd_reg.predict(X_test)
y_pred_stoc

import pandas as pd

# Reshape y_pred_reg_valid to have the same number of features as the original data used for fitting std_data
# Assuming std_data was fitted on data with 7 features
num_original_features = std_data.n_features_in_  # Get the number of features used during fitting
original_y_pred = std_data.inverse_transform(
    np.concatenate(
        [
            y_pred_stoc.reshape(-1, 1),  # Reshape predictions to a single column
            np.zeros((y_pred_stoc.shape[0], num_original_features - 1)),  # Pad with zeros for other features
        ],
        axis=1,
    )
)

# Extract the 'count' column, which should be the first column after inverse transform
df = pd.DataFrame({"y_pred_reg_valid": original_y_pred[:, 0]})
df.to_csv("predictions_stoc.csv", index=False)

plt.figure(figsize=(10, 6))
plt.plot(range(epochs), losses_sgd, alpha=0.6, label="Stochastic Gradient Descent")
plt.plot(range(epochs), losses_batch,label="Batch Gradient Descent")
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Gradient Descent Comparison')
plt.legend()
plt.show()

"""#Polynomial Regression"""

from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree=1, include_bias=True)
X_poly = poly_reg.fit_transform(X_train)
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y_train)
lin_reg.intercept_, lin_reg.coef_

X_poly_test = poly_reg.fit_transform(X_test)
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y_train)
y_pred_poly = lin_reg.predict(X_poly_test)

import pandas as pd

# Get the number of features used during fitting
num_original_features = std_data.n_features_in_

# No need to reshape if y_pred_poly is already 1-dimensional
# y_poly_test_valid_reshaped = y_pred_poly[:, :num_original_features]

# Instead, create a DataFrame directly from y_pred_poly
original_y_pred = std_data.inverse_transform(
    np.concatenate(
        [
            y_pred_poly.reshape(-1, 1),  # Reshape predictions to a single column
            np.zeros((y_pred_poly.shape[0], num_original_features - 1)),  # Pad with zeros for other features
        ],
        axis=1,
    )
)

# Extract the predictions, which should be in the first column
# after inverse_transform
df = pd.DataFrame({"y_pred_reg_valid": original_y_pred[:, 0]})
df.to_csv("predictions_poly.csv", index=False)

"""# Using regularised linear regression to give good bias values

"""

#Ridge Regression
from sklearn.linear_model import Ridge

alphas = np.logspace(-3, 1, 50)
ridge_coefs = []
for alp in alphas:
  ridge_reg = Ridge(alpha=alp, solver="cholesky", random_state=42)
  ridge_reg.fit(X_train, y_train)
  ridge_coefs.append(ridge_reg.coef_[0])
  y_pred_reg = ridge_reg.predict(X_test)
  print("Mean Absolute Error:",mean_absolute_error(y_test_reshaped,y_pred))
  print("Mean Squared Error:",mean_squared_error(y_test_reshaped,y_pred))
  print("Root Mean Squared Error:",np.sqrt(mean_squared_error(y_test_reshaped,y_pred)))
  print("R2 Score:",r2_score(y_test,y_pred))

plt.figure(figsize=(10, 6))
plt.plot(alphas, ridge_coefs)
plt.xscale('log')
plt.xlabel('Regularization Parameter (alpha)')
plt.ylabel('Coefficients')
plt.title('Ridge Regularization Methods Comparison')
plt.legend()
plt.show()

from sklearn.linear_model import Ridge

alphas = np.logspace(-3, 1, 50)
for alp in alphas:
  ridge_reg = Ridge(alpha=alp, solver="cholesky", random_state=42)
  ridge_reg.fit(X_train, y_train)
  y_pred_reg = ridge_reg.predict(X_test)
y_pred_reg

import pandas as pd

# Reshape y_pred_reg_valid to have the same number of features as the original data used for fitting std_data
# Assuming std_data was fitted on data with 7 features
num_original_features = std_data.n_features_in_  # Get the number of features used during fitting
original_y_pred = std_data.inverse_transform(
    np.concatenate(
        [
            y_pred_reg.reshape(-1, 1),  # Reshape predictions to a single column
            np.zeros((y_pred_reg.shape[0], num_original_features - 1)),  # Pad with zeros for other features
        ],
        axis=1,
    )
)

# Extract the 'count' column, which should be the first column after inverse transform
df = pd.DataFrame({"y_pred_reg_valid": original_y_pred[:, 0]})
df.to_csv("predictions_ridge.csv", index=False)